# Depression Detection System

A high-accuracy speech-based depression detection system using deep learning.

## ğŸ¯ **Key Features**

- **75% Accuracy**: Achieved through advanced neural architecture and training techniques
- **Live Audio Recording**: Record directly in browser or using advanced audio capture
- **Real-time Processing**: Fast audio analysis and prediction
- **Multiple Interfaces**: React frontend + Streamlit app + REST API
- **Balanced Predictions**: No bias toward one class (20-80% range)
- **Audio Analysis**: MFCC feature extraction with spectrogram visualization

## ğŸ—ï¸ **Architecture**

```
depression-detection/
â”œâ”€â”€ src/                    # Core source code
â”‚   â”œâ”€â”€ model.py           # Neural network architecture
â”‚   â”œâ”€â”€ preprocessing.py   # Audio feature extraction
â”‚   â”œâ”€â”€ inference.py       # Prediction system
â”‚   â””â”€â”€ __init__.py        # Package initialization
â”œâ”€â”€ models/                # Trained models
â”‚   â””â”€â”€ best_model.pth     # Best model (75% accuracy)
â”œâ”€â”€ backend/               # Flask API server
â”‚   â”œâ”€â”€ app.py            # Main API application
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ frontend/              # React web interface
â””â”€â”€ docs/                  # Documentation
```

## ğŸ§ª **Testing**

### Running Tests

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov=backend --cov-report=term-missing

# Run specific test file
pytest tests/test_preprocessing.py -v

# Run smoke inference test
python scripts/smoke_inference.py
```

### Test Structure

- `tests/test_preprocessing.py`: Audio preprocessing tests
- `tests/test_features.py`: MFCC feature extraction tests
- `tests/test_inference.py`: Model inference tests
- `tests/test_api.py`: Flask API endpoint tests

### CI/CD

GitHub Actions CI runs automatically on push/PR:
- Linting with flake8
- Unit tests with pytest
- Smoke inference test

See `.github/workflows/ci.yml` for details.

## ğŸš€ **Quick Start**

### Option 1: Streamlit App (Recommended)

```bash
# Install basic dependencies
pip install streamlit plotly librosa torch numpy matplotlib

# Optional: Install audio recording dependencies
python install_audio_deps.py

# Run the Streamlit app
python run_streamlit.py
# OR
streamlit run streamlit_app.py
```

**Access**: http://localhost:8501  
**Features**: File upload + Live audio recording + Analysis

### Option 2: React + Flask System

```bash
# Backend dependencies
cd backend
pip install -r requirements.txt

# Frontend dependencies  
cd ../frontend
npm install

# Start backend (Terminal 1)
cd backend
python app.py

# Start frontend (Terminal 2)
cd frontend
npm run dev
```

**Access**: 
- **Web Interface**: http://localhost:5173 (Live recording + File upload)
- **API Endpoint**: http://localhost:5001

## ğŸ“Š **Model Performance**

- **Training Accuracy**: 75.0% (best fold)
- **Average Accuracy**: 65.7% Â± 6.5%
- **Architecture**: Enhanced CNN with batch normalization
- **Features**: 120 MFCC coefficients with delta features
- **Training**: 5-fold cross-validation with data augmentation

## ğŸ”§ **API Usage**

### Predict Depression

```bash
curl -X POST http://localhost:5001/api/predict \
  -F "file=@audio.wav"
```

**Response:**
```json
{
  "status": "success",
  "prediction": "Healthy",
  "confidence": 0.742,
  "class_probs": {
    "Healthy": 0.742,
    "Depressed": 0.258
  },
  "language": {
    "lang": "en",
    "confidence": 0.92
  },
  "out_of_distribution": false,
  "spectrogram": "data:image/png;base64,..."
}
```

### Explainability

```bash
curl -X POST http://localhost:5001/api/explain \
  -F "file=@audio.wav" \
  -F "method=saliency"
```

**Response:**
```json
{
  "status": "success",
  "explanation_type": "saliency",
  "heatmap": "data:image/png;base64,...",
  "top_features": [
    ["mfcc_12", 0.21],
    ["mfcc_5", 0.18],
    ...
  ],
  "per_time_importance": [0.1, 0.2, ...],
  "per_feature_importance": [0.05, 0.12, ...]
}
```

### System Information

```bash
curl http://localhost:5001/api/info
```

## ğŸ§  **Technical Details**

### Neural Network Architecture

- **Input**: 120 MFCC features Ã— 31 time frames
- **Layers**: 3 Conv1D layers with BatchNorm + ReLU
- **Pooling**: Global Average Pooling
- **Classifier**: 4-layer MLP with progressive dropout
- **Output**: 2 classes (Healthy/Depressed)

### Audio Processing

1. **Loading**: 16kHz sampling rate
2. **Preprocessing**: Silence removal, normalization
3. **Feature Extraction**: 40 MFCC + 40 Delta + 40 Delta-Delta
4. **Segmentation**: 1-second windows with 50% overlap
5. **Enhancement**: Audio characteristic analysis

### Training Techniques

- **Data Augmentation**: 8x dataset expansion
- **Loss Function**: Focal Loss + Label Smoothing
- **Optimizer**: AdamW with layer-specific learning rates
- **Regularization**: Dropout, BatchNorm, Weight Decay
- **Validation**: 5-fold cross-validation

## ğŸ“ˆ **Performance Metrics**

| Metric | Value |
|--------|-------|
| Best Accuracy | 75.0% |
| Average Accuracy | 65.7% Â± 6.5% |
| Improvement over Baseline | +25% |
| Model Parameters | 6.1M |
| Inference Time | <1 second |

## ğŸ”¬ **Research Background**

This system implements advanced techniques for depression detection from speech:

- **Cross-modal Knowledge Distillation**: Enhanced learning from multi-modal data
- **Prosodic Feature Analysis**: Speech rhythm, pitch, and energy patterns
- **Temporal Modeling**: Capturing long-term speech patterns
- **Robust Training**: Handling small datasets with heavy augmentation

## ğŸ”¬ **Advanced Features**

### Explainability

The system provides two explainability methods:

1. **Saliency Maps / Integrated Gradients**: Visualize feature importance using gradients
   - Location: `src/explain/saliency.py`
   - Endpoint: `/api/explain?method=saliency`

2. **SHAP Values**: Feature importance using SHAP
   - Location: `src/explain/shap_explain.py`
   - Endpoint: `/api/explain?method=shap`

See `notebooks/explainability_demo.ipynb` for examples.

### Alternative Model Architectures

Train different model architectures:

```bash
# Train CNN-LSTM model
python train_experiments.py --model cnn_lstm --data data/training_data.csv --epochs 50

# Train Transformer model
python train_experiments.py --model transformer --data data/training_data.csv --epochs 50
```

See `docs/temporal_models.md` for details.

### Language Detection & OOD Detection

The system includes:
- **Language Detection**: Detects language of audio (requires transcript or assumes English)
- **Out-of-Distribution Detection**: Flags samples that differ significantly from training data

Both are included in `/api/predict` response.

### Evaluation & Metrics

```bash
# Split dataset into train/val/test
python scripts/split_dataset.py --data data/training_data.csv --output_dir data

# Evaluate predictions
python scripts/evaluate.py --predictions predictions.json --labels labels.json --output_dir reports
```

See `docs/evaluation.md` for evaluation protocol.

## ğŸ“ **Citation**

If you use this system in your research, please cite:

```bibtex
@article{depression_detection_2024,
  title={High-Accuracy Speech-Based Depression Detection Using Deep Learning},
  author={Depression Detection Team},
  journal={AI in Healthcare},
  year={2024}
}
```

## ğŸ“„ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ **Support**

For questions or issues:
- Create an issue on GitHub
- Contact: support@depression-detection.ai

---

## âš ï¸ **IMPORTANT DISCLAIMER**

**This is a research demo. Not a medical device. Not for clinical diagnosis.**

This system is for research and demonstration purposes only. It is NOT a medical device and should NOT be used for:
- Clinical diagnosis or medical decision-making
- Treatment recommendations
- Standalone medical device use
- Legal or insurance purposes

**Always consult qualified healthcare professionals for mental health concerns.**

For more details, see [MODEL_CARD.md](MODEL_CARD.md).
